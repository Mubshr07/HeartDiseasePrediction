{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os \n",
    "from sklearn.datasets import load_files \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "#import cudf\n",
    "#import cupy as cp\n",
    "#from cuml.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, StackingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF  \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb  \n",
    " \n",
    "import random \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fileData: (37079, 40)\n",
      "Column Headings: Index(['Gender', 'Age', 'X60-sec-pulse', 'Systolic', 'Diastolic', 'Weight',\n",
      "       'Height', 'Body-Mass-Index', 'White-Blood-Cells', 'Lymphocyte',\n",
      "       'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells', 'Hemoglobin',\n",
      "       'Platelet-count', 'Segmented-Neutrophils', 'Hematocrit', 'Albumin',\n",
      "       'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose', 'Iron',\n",
      "       'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Moderate-work', 'Diabetes', 'Blood-Rel-Diabetes', 'Blood-Rel-Stroke',\n",
      "       'CoronaryHeartDisease'],\n",
      "      dtype='object')\n",
      "Number of Records: 37079\n",
      "\n",
      "Number of Missing Values: 0\n",
      "Number of duplicate records removed: 0\n",
      "Shape of fileData: (37079, 40)\n",
      "Shape of fileData End: (37079, 40)\n",
      "\n",
      "\n",
      "columns of x:: 39 \n",
      "\n",
      " and features of X: Index(['Gender', 'Age', 'X60-sec-pulse', 'Systolic', 'Diastolic', 'Weight',\n",
      "       'Height', 'Body-Mass-Index', 'White-Blood-Cells', 'Lymphocyte',\n",
      "       'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells', 'Hemoglobin',\n",
      "       'Platelet-count', 'Segmented-Neutrophils', 'Hematocrit', 'Albumin',\n",
      "       'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose', 'Iron',\n",
      "       'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Moderate-work', 'Diabetes', 'Blood-Rel-Diabetes', 'Blood-Rel-Stroke'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    " \n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "Shape of fileData: (37079, 40) , target Len:37079\n",
      "X: (37079, 39) , Y:(37079,)\n",
      "Target Column Name:: CoronaryHeartDisease \n",
      "\n",
      "Class distribution after undersampling:\n",
      "0    1056\n",
      "1    1056\n",
      "Name: CoronaryHeartDisease, dtype: int64\n",
      "\n",
      " X Train: Shape:: (2112, 39)\n",
      " X Test: Shape:: (904, 39)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    " \n",
    "print(\"***************************************\") \n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "# Undersample the majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "'''\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "'''\n",
    "\n",
    "# Print the class distribution after undersampling\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))\n",
    "\n",
    "'''\n",
    "print(\"\\n X Train: Shape::\\n {}\".format(X_train.shape))\n",
    "print(\"\\n X Train: head::\\n {}\".format(X_train.columns))\n",
    "print(\"\\n X Test: head:: \\n{}\".format(X_test.columns))\n",
    "print(\"\\n Y Train: shape::\\n {}\".format(y_train.shape)) \n",
    "print(\"\\n Y Test: shape::\\n {}\".format(y_test.shape)) \n",
    "'''\n",
    "#print(\"\\n X Train: Info::\\n {}\".format(X_train.info())) \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "instance = np.array(X_test)  # Example: explaining the first instance in the dataset\n",
    " \n",
    "dataSetResultDirectory += (\"DatasetResults_PureML_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataSet Positive Class Records:: 1056\n",
      "Train DataSet Negative Class Records:: 1056\n",
      "Train DataSet Total Records:: 2112\n",
      "\n",
      "\n",
      "\n",
      "Test DataSet Positive Class Records:: 452\n",
      "Test DataSet Negative Class Records:: 452\n",
      "Test DataSet Total Records:: 904\n"
     ]
    }
   ],
   "source": [
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrixPic(fileID, method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_predictions(y_test,predicted_Y, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    \n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ') \n",
    " \n",
    "    accuracyValue = (accuracy_score(y_test, predicted_Y)*100.0) \n",
    "    recallValue = (recall_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "    precisionValue = (precision_score(y_test, predicted_Y) * 100.0) \n",
    "    f1Score = (f1_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "\n",
    " \n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(fileID)\n",
    "    singleRowInTable.append(method)\n",
    "    singleRowInTable.append(\"{:.2f}\".format(accuracyValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(recallValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(precisionValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(f1Score) )\n",
    "    singleRowInTable.append(\"-\")\n",
    "\n",
    "    finalResultTable.append((singleRowInTable) )\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy : {:.2f} %\".format( accuracyValue) \n",
    "    recallString =  'Recall : {:.2f} %'.format(recallValue)\n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    f1String = 'F1 Score : {:.2f} %'.format(f1Score) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "    \n",
    "    numberOfTrainingRecords = \"No of Training Records: {}  {:.2f} %\".format(len(X_train), ((len(X_train)/totalRecords) * 100.0))\n",
    "    numberOfTestingRecords = \"No of Testing Records: {}  {:.2f} %\".format(len(X_test), ((len(X_test)/totalRecords) * 100.0))\n",
    "\n",
    "\n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False , fontsize=12 )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.21,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.25,  numberOfTrainingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.29,  numberOfTestingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    \n",
    "    '''\n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    '''\n",
    "    \n",
    "    picturePath = \"{}{}.Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, fileID, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "\n",
    "def makeConfusionMatrixPicWithProbibality(fileID, method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_predictions(y_test,predicted_Y, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    \n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ') \n",
    "\n",
    "    predicted_Proba = classifierObj.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predicted_Proba)\n",
    "    roc_auc = auc(fpr, tpr) \n",
    "\n",
    "    accuracyValue = (accuracy_score(y_test, predicted_Y)*100.0) \n",
    "    recallValue = (recall_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "    precisionValue = (precision_score(y_test, predicted_Y) * 100.0) \n",
    "    f1Score = (f1_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "\n",
    " \n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(fileID)\n",
    "    singleRowInTable.append(method)\n",
    "    singleRowInTable.append(\"{:.2f}\".format(accuracyValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(recallValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(precisionValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(f1Score) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(roc_auc) )\n",
    "\n",
    "    finalResultTable.append((singleRowInTable) )\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy : {:.2f} %\".format( accuracyValue) \n",
    "    recallString =  'Recall : {:.2f} %'.format(recallValue)\n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    f1String = 'F1 Score : {:.2f} %'.format(f1Score) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "    \n",
    "    numberOfTrainingRecords = \"No of Training Records: {}  {:.2f} %\".format(len(X_train), ((len(X_train)/totalRecords) * 100.0))\n",
    "    numberOfTestingRecords = \"No of Testing Records: {}  {:.2f} %\".format(len(X_test), ((len(X_test)/totalRecords) * 100.0))\n",
    "\n",
    "\n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False , fontsize=12 )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.21,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.25,  numberOfTrainingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.29,  numberOfTestingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    \n",
    "    '''\n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    '''\n",
    "    \n",
    "    picturePath = \"{}{}.Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, fileID, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "\n",
    "\n",
    "def evaluation(y_test, predicted_Y):\n",
    "    accuracyValue = (accuracy_score(y_test, predicted_Y)*100.0) \n",
    "    recallValue = (recall_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "    precisionValue = (precision_score(y_test, predicted_Y) * 100.0) \n",
    "    f1Score = (f1_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "\n",
    "\n",
    "    print(\"Accuracy : {:.3f} %\".format( accuracyValue) )\n",
    "    print('Recall : {:.3f} %'.format(recallValue))\n",
    "    print('Precision : {:.3f} %'.format(precisionValue)  )\n",
    "    print('F1 Score : {:.3f} %'.format(f1Score)  )\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 74.447 %\n",
      "Recall : 74.447 %\n",
      "Precision : 64.381 %\n",
      "F1 Score : 74.708 %\n"
     ]
    }
   ],
   "source": [
    "#6.     Naive Bayes (with appropriate modifications for numerical data, such as Gaussian Naive Bayes)\n",
    "classifierNB = GaussianNB(var_smoothing=0.00001 )\n",
    "classifierNB.fit(X_train, y_train)\n",
    "predicted_NB = classifierNB.predict(X_test)\n",
    "\n",
    "evaluation(predicted_NB, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 74.115 %\n",
      "Recall : 74.115 %\n",
      "Precision : 65.265 %\n",
      "F1 Score : 74.319 %\n"
     ]
    }
   ],
   "source": [
    "#6.     Naive Bayes (with appropriate modifications for numerical data, such as Gaussian Naive Bayes)\n",
    "classifierNB = GaussianNB(var_smoothing=0.00001 )\n",
    "classifierNB.fit(X_train_scaled, y_train)\n",
    "predicted_NB = classifierNB.predict(X_test_scaled)\n",
    "\n",
    "evaluation(predicted_NB, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
