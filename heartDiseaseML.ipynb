{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, StackingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier , RidgeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis  \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb  \n",
    "\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 4\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0)\n",
    "# 2. Handling Outliers:\n",
    "# 3. Feature Scaling:\n",
    "# 4. Encoding Categorical Variables:\n",
    "# 5. Feature Engineering:\n",
    "# 6. Handling Skewed Distributions:\n",
    "# 7. Dimensionality Reduction:\n",
    "# 8. Handling Imbalanced Classes:\n",
    "# 9. Normalization of Data:\n",
    "# 10. Data Splitting:\n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score'], ] \n",
    "'''\n",
    "dataSetResultDirectory += (\"DatasetResults_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n",
    "'''\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCorrelationPic(correlationMatrix, numberOfTopFeatures, targetColumnName):     \n",
    "    correlation_values = correlationMatrix.abs()\n",
    "    sorted_correlation = correlation_values.unstack().sort_values(ascending=False)\n",
    "    sorted_correlation = sorted_correlation[sorted_correlation != 1.0]\n",
    "\n",
    "    num_features = numberOfTopFeatures  # Number of top features to display\n",
    "    top_features = sorted_correlation.head(num_features)\n",
    "    print(\"Top\", num_features, \"features based on correlation:\")\n",
    "    print(top_features)\n",
    " \n",
    "    top_features = correlationMatrix.abs().nlargest(numberOfTopFeatures, targetColumnName)[targetColumnName].index\n",
    "    top_correlation_matrix = correlationMatrix.loc[top_features, top_features]\n",
    "\n",
    "    mplot.figure(figsize=(10, 8))\n",
    "    sns.heatmap(top_correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    # Set the title of the plot\n",
    "    mplot.title('Correlation Heatmap ({})'.format(dataSetName)  ,fontsize=16, fontweight='bold')\n",
    "    \n",
    "    picturePath = \"{}0.1_Correlation_Matrix_DateSetName_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "def plot_classification_report(title, dataSetName, y_tru, y_prd, figsize=(6, 6), ax=None):\n",
    "    #mplot.figure(figsize=figsize)\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = ['Healthy', 'Heart Disease']\n",
    "    rep = np.array( precision_recall_fscore_support(y_tru, y_prd) ).T\n",
    "    rep[0][0] *= 100.0\n",
    "    rep[0][1] *= 100.0\n",
    "    rep[0][2] *= 100.0\n",
    "    rep[1][0] *= 100.0\n",
    "    rep[1][1] *= 100.0\n",
    "    rep[1][2] *= 100.0\n",
    "    \n",
    "    ax = sns.heatmap(rep, annot=True, cmap='Blues', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
    "    ax.set_title(\"Classification Report {} Model\\n\\n\".format(title) ,fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('\\nDataset:{}'.format(dataSetName))\n",
    "    ax.xaxis.set_ticklabels(xticks)\n",
    "    ax.set_ylabel('Classes')\n",
    "    ax.yaxis.set_ticklabels(yticks)\n",
    "    \n",
    "    picturePath = \"{}ClassificationReport_{}_{}.png\".format(dataSetResultDirectory, title, dataSetName) \n",
    "    mplot.savefig(picturePath, dpi=300, bbox_inches='tight')\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "def makeConfusionMatrixPic(fileID, method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_estimator(classifierObj, X_test, y_test, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "    accuracyValue = (accuracy_score(y_test, predicted_Y)*100.0) \n",
    "    recallValue = (recall_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "    precisionValue = (precision_score(y_test, predicted_Y) * 100.0) \n",
    "    f1Score = (f1_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "\n",
    " \n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(fileID)\n",
    "    singleRowInTable.append(method)\n",
    "    singleRowInTable.append(\"{:.2f}\".format(accuracyValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(recallValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(precisionValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(f1Score) )\n",
    "\n",
    "    finalResultTable.append((singleRowInTable) )\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy : {:.2f} %\".format( accuracyValue) \n",
    "    recallString =  'Recall : {:.2f} %'.format(recallValue)\n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    f1String = 'F1 Score : {:.2f} %'.format(f1Score) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "\n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False , fontsize=12 )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.21,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    \n",
    "    '''\n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    '''\n",
    "    \n",
    "    picturePath = \"{}{}.Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, fileID, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "def makeLIMEreport(method, classifierObj):\n",
    "    class_names =  ['Healthy', 'Heart Disease']\n",
    "    listOfFeatures =  classifierObj.feature_names_in_\n",
    "    print(\"\\n\\n\\n Obj Features: {}, len: {} \\n\\n\".format(listOfFeatures, len(listOfFeatures)))\n",
    "    #explainer = lime_tabular.LimeTabularExplainer(np.array(X_train) , mode='regression', feature_names=listOfFeatures, verbose=True, class_names=class_names )\n",
    "    explainer = lime_tabular.LimeTabularExplainer(np.array(X_train) , mode='classification', feature_names=listOfFeatures, verbose=True, class_names=class_names )\n",
    "\n",
    "    num_instances = int(len(X_test)/4)\n",
    "    print(\"num_instances: {}\".format(num_instances))\n",
    "    # Generate explanations for each testing instance\n",
    "    explanations = []\n",
    "    for i in range(num_instances):\n",
    "        instan = instance[i]\n",
    "        explanation = explainer.explain_instance(instan, classifierObj.predict_proba, num_features=len(instan))\n",
    "        explanations.append(explanation)\n",
    "\n",
    "    #Extract the feature importance values from the explanations\n",
    "    feature_importances = np.mean([exp.as_map()[1] for exp in explanations], axis=0)\n",
    "    averageF = []\n",
    "    for singleFeature in feature_importances:\n",
    "        averageF.append(singleFeature[0])\n",
    "     \n",
    "    print(\"\\n\\nfeature_list:{}\".format(listOfFeatures))\n",
    "    print(\"averageF:{} \\n\\n\".format(averageF))\n",
    "\n",
    "\n",
    "    fig, ax = mplot.subplots()\n",
    "    ax.barh(listOfFeatures, averageF )\n",
    "    ax.set_ylabel('Feature List')\n",
    "    ax.set_xlabel('Average Importance')\n",
    "    ax.set_title(\"XAI LIME ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "     \n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(X_test))\n",
    "    explainerModelString = \"LIME Explainer Model: {}\".format(explainer.mode)\n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  explainerModelString, horizontalalignment='left', wrap=False )   \n",
    " \n",
    "\n",
    "    picturePath = \"{}XAI_LIME_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "def makeSHAPreport(method, model):\n",
    "    shap_explainer = shap.Explainer(model, X_train) \n",
    "    shap_values = shap_explainer.shap_values(X_test)  \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP Explainer ({} Model)\".format(method) ,fontsize=16, fontweight='bold')     \n",
    "\n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(X_test))\n",
    "    shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "    picturePath = \"{}XAI_SHAP_Explainer_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "def makeSHAP_KERNELreport(method, model):\n",
    "    shap_explainer = shap.KernelExplainer(model.predict_proba, X_train) \n",
    "    testingShape = X_test[0:10]\n",
    "    shap_values = shap_explainer.shap_values(testingShape)  \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP KernelExplainer ({} Model)\".format(method) ,fontsize=16, fontweight='bold')     \n",
    "\n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(testingShape))\n",
    "    shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "    picturePath = \"{}XAI_SHAP_KernelExplainer_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columnsForGraph = []\n",
    "columnsForGraph.clear()\n",
    "tableDataRow = []\n",
    "for column in columns:\n",
    "    singleColumnCount = fileData[column].value_counts()\n",
    "    if(len(singleColumnCount) < 3):\n",
    "        #print('Column Name:{} -> total records:{}'.format(column, totalRecords ) )\n",
    "        #print('Number of classes:', len(singleColumnCount))\n",
    "        #print('Class distribution:')\n",
    "        #print(singleColumnCount)\n",
    "        #print(\"np Array: {}\".format(np.array(singleColumnCount)))\n",
    "        #print(\"index: 0: {} -> {} %\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100))\n",
    "        #print(\"index: 1: {} -> {} %\".format(np.array(singleColumnCount)[1], ( np.array(singleColumnCount)[1] /totalRecords) * 100))  \n",
    "        #print('---------------------------------------------------------------')\n",
    "        columnsForGraph.append(column)\n",
    " \n",
    "tableDataRow = [ ['Index', 'Column Name', 'Total Classes','Class A Records','Class B Records'], ]\n",
    "\n",
    "indexx = 1\n",
    "for column in columnsForGraph:\n",
    "    singleColumnCount = fileData[column].value_counts()\n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(indexx)\n",
    "    singleRowInTable.append(column)\n",
    "    singleRowInTable.append(len(singleColumnCount))\n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[0], (np.array(singleColumnCount)[0] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    cellDataString = \"{} -> {:.2f}%\".format(np.array(singleColumnCount)[1], (np.array(singleColumnCount)[1] /totalRecords) * 100)\n",
    "    singleRowInTable.append((cellDataString)) \n",
    "    indexx += 1\n",
    "    tableDataRow.append(singleRowInTable) \n",
    "\n",
    " \n",
    "# Determine the number of rows in the table (excluding the header)\n",
    "num_rows = len(tableDataRow) + 1\n",
    "# Calculate the desired figure size based on the number of rows\n",
    "fig_width = 6  # Set the desired width of the figure\n",
    "fig_height = num_rows * 0.5  # Adjust the scaling factor to control the height\n",
    "\n",
    "fig, ax = mplot.subplots(figsize=(fig_width, fig_height)) \n",
    "table = mplot.table(cellText=tableDataRow, loc='center') \n",
    "\n",
    "table.auto_set_column_width(col=list(range(len(tableDataRow[0]))))\n",
    "\n",
    "\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, fileData.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(fileData[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA =\"Class A Records: {} , {:.2f} %\".format(np.array(fileData[columns[-1]].value_counts())[0], (np.array(fileData[columns[-1]].value_counts())[0] /totalRecords) * 100)\n",
    "distributionOfTargetClassB =\"Class B Records: {} , {:.2f} %\".format(np.array(fileData[columns[-1]].value_counts())[1], (np.array(fileData[columns[-1]].value_counts())[1] /totalRecords) * 100)\n",
    "\n",
    "fig.text(-0.1, +0.10,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.02,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, -0.06,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.14,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "\n",
    "remarks = \"You need to distribute the target class in equal number of records in training-set.\"\n",
    "#fig.text(-0.2, -0.15,  remarks, horizontalalignment='left', wrap=True ,fontsize=12, fontweight='bold' )   \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Exploring Dataset - {dataSetName}' ,fontsize=16, fontweight='bold') \n",
    "picturePath = \"{}0.DataSet_analysis_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig(picturePath,  dpi=300 )\n",
    "mplot.show()\n",
    "mplot.close()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix \n",
    "correlation_matrix = fileData.corr()\n",
    "#makeCorrelationPic(correlation_matrix, 15, 'CoronaryHeartDisease') \n",
    "\n",
    "makeCorrelationPic(correlation_matrix, 15, fileData.__dataframe__().column_names()[-1] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "column_names = X.columns\n",
    "\n",
    "#print(\"columns of x:: {} \\n and features of X: {} \\n\\n\".format(len(column_names), column_names))\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(X, Y)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Get the selected column indices\n",
    "selected_columns_indices = pca.components_ #.argsort()[:-1][:pca.n_components_]\n",
    "\n",
    "# Get the selected column names\n",
    "#selected_columns = column_names[selected_columns_indices]\n",
    "\n",
    "pd2 = pd.DataFrame(X_pca, columns=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\" ])\n",
    "#pd3 = pd.DataFrame(Y, columns=[fileData.__dataframe__().column_names()[-1]]) \n",
    "pd2[fileData.__dataframe__().column_names()[-1]] = Y \n",
    "\n",
    "#pd2 = pd2.add(pd3)\n",
    "#pd2 = pd2 + pd3\n",
    "\n",
    "pd2.info()\n",
    "\n",
    "# Print the selected column names\n",
    "print(f' len: {len(selected_columns_indices)} ')\n",
    "print(f' len: {len(selected_columns_indices[0])} ')\n",
    "#print(selected_columns)\n",
    "\n",
    "# Print the selected column names\n",
    "#print(f'\\n\\n len: {len(selected_columns[5])} -> col: {selected_columns[5]} \\n') \n",
    "\n",
    "X = pd2.drop('CoronaryHeartDisease', axis=1)  # Features\n",
    "Y = pd2['CoronaryHeartDisease']  # Labels\n",
    "\n",
    "columns = pd2.__dataframe__().column_names() \n",
    "totalRecords = (pd2.__dataframe__().num_rows())\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score'], ] \n",
    "\n",
    "dataSetResultDirectory += (\"DatasetResults_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "\n",
    "dataSetName += \" {}\".format(pd2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    " \n",
    "print(\"***************************************\") \n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "\n",
    "# Undersample the majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the class distribution after undersampling\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))\n",
    "\n",
    "'''\n",
    "print(\"\\n X Train: Shape::\\n {}\".format(X_train.shape))\n",
    "print(\"\\n X Train: head::\\n {}\".format(X_train.columns))\n",
    "print(\"\\n X Test: head:: \\n{}\".format(X_test.columns))\n",
    "print(\"\\n Y Train: shape::\\n {}\".format(y_train.shape)) \n",
    "print(\"\\n Y Test: shape::\\n {}\".format(y_test.shape)) \n",
    "'''\n",
    "#print(\"\\n X Train: Info::\\n {}\".format(X_train.info())) \n",
    "\n",
    "\n",
    "instance = np.array(X_test)  # Example: explaining the first instance in the dataset\n",
    " \n",
    " \n",
    "dataSetResultDirectory += (\"DatasetResults_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n Y Train: shape:: {}\".format(y_train.shape)) \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "\n",
    "\n",
    "print(\" Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\" Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\" Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.     Logistic Regression\n",
    "#classifierLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, l1_ratio=None, max_iter=100,multi_class='warn', n_jobs=None, penalty='l2',random_state=0, solver='liblinear', tol=0.0001, verbose=0,warm_start=False)\n",
    "classifierLR = LogisticRegression(solver='liblinear', random_state=0)\n",
    "#classifierLR = LogisticRegression()\n",
    "classifierLR.fit(X_train, y_train)\n",
    "predicted_LR = classifierLR.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifierLR.score(X_test, y_test)))\n",
    "\n",
    "methodName = \"LogisticRegression\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_LR)  \n",
    "makeConfusionMatrixPic(1, methodName, dataSetName, classifierLR , X_test, y_test, predicted_LR)\n",
    "#makeSHAPreport(methodName, classifierLR)\n",
    "#makeSHAP_KERNELreport(methodName, classifierLR)\n",
    "#makeLIMEreport(methodName, classifierLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.     Decision Trees\n",
    "classifierDT = DecisionTreeClassifier() \n",
    "classifierDT.fit(X_train, y_train) \n",
    "predicted_DT = classifierDT.predict(X_test)\n",
    "\n",
    "\n",
    "methodName = \"DecisionTree\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_DT)  \n",
    "makeConfusionMatrixPic(2, methodName, dataSetName, classifierDT , X_test, y_test, predicted_DT)\n",
    "#makeSHAPreport(methodName, classifierDT)\n",
    "#makeSHAP_KERNELreport(methodName, classifierDT)\n",
    "#makeLIMEreport(methodName, classifierDT)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_DT)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.     Random Forests\n",
    "classifierRF = RandomForestClassifier(n_estimators=100, random_state = 42)\n",
    "classifierRF.fit(X_train, y_train)\n",
    "y_PredictionRF = classifierRF.predict(X_test)\n",
    "\n",
    "methodName = \"RF\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, y_PredictionRF)  \n",
    "makeConfusionMatrixPic(3, methodName, dataSetName, classifierRF , X_test, y_test, y_PredictionRF)\n",
    "#makeSHAPreport(methodName, classifierRF)\n",
    "#makeSHAP_KERNELreport(methodName, classifierRF)\n",
    "#makeLIMEreport(methodName, classifierRF)\n",
    "  \n",
    "'''\n",
    "confusionM_rf = confusion_matrix(y_test, y_PredictionRF)\n",
    "print(\" Random Forest Accuracy:\", accuracy_score(y_test, y_PredictionRF))\n",
    "print(\" Random Forest ConfusionM: \", confusionM_rf)\n",
    "print(\" Random Forest  length of estimators:{}\".format(len(classifierRF.estimators_)))\n",
    "\n",
    "\n",
    "folderPath = r'./RandomForestTrees_{}/'.format(dataSetName)\n",
    "print(\"FolderPath:{}\".format(folderPath))\n",
    "if not os.path.isdir(folderPath):\n",
    "    os.makedirs(folderPath)\n",
    "\n",
    "#for indx in range(len(rfClassifier.estimators_)):\n",
    "for indx in range(5):\n",
    "    tree = classifierRF.estimators_[indx]\n",
    "    dot_data = export_graphviz(tree, out_file=None, feature_names=classifierRF.feature_names_in_, rounded=True, precision=1)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    picturePath = \"{}tree_{}.png\".format(folderPath, indx)\n",
    "    graph.write_png(picturePath)\n",
    "    print(\"Random Forest Tree Picture Generated:: path:\", picturePath)\n",
    "    #os.startfile(picturePath)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.     Support Vector Machines (SVM)\n",
    "classifierSVM = SVC(kernel = 'rbf', probability=True, random_state = 0)\n",
    "#classifierSVM = SVC(kernel = 'rbf', probability=True)\n",
    "#classifierSVM = SVC(C= 0.05, gamma=1/41, kernel='rbf', class_weight='balanced')\n",
    "\n",
    "classifierSVM.fit(X_train, y_train)\n",
    "y_PredictionSVM = classifierSVM.predict(X_test)\n",
    "\n",
    "methodName = \"SVM\"\n",
    "#plot_classification_report(methodName, dataSetName, y_test, y_PredictionSVM)  \n",
    "makeConfusionMatrixPic(4, methodName, dataSetName, classifierSVM , X_test, y_test, y_PredictionSVM)\n",
    "#makeSHAPreport(methodName, classifierSVM)\n",
    "#makeSHAP_KERNELreport(methodName, classifierSVM)\n",
    "#makeLIMEreport(methodName, classifierSVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.     K-Nearest Neighbors (KNN)\n",
    "classifierKNN = KNN(n_neighbors=3)\n",
    "classifierKNN.fit(X_train, y_train)\n",
    "y_PredictionKNN = classifierKNN.predict(X_test)\n",
    "\n",
    "methodName = \"KNN\"\n",
    "#plot_classification_report(methodName, dataSetName, y_test, y_PredictionKNN)  \n",
    "makeConfusionMatrixPic(5, methodName, dataSetName, classifierKNN , X_test, y_test, y_PredictionKNN)\n",
    "#makeSHAPreport(methodName, classifierKNN)\n",
    "#makeSHAP_KERNELreport(methodName, classifierKNN)\n",
    "#makeLIMEreport(methodName, classifierKNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.     Naive Bayes (with appropriate modifications for numerical data, such as Gaussian Naive Bayes)\n",
    "classifierNB = GaussianNB()\n",
    "classifierNB.fit(X_train, y_train)\n",
    "predicted_NB = classifierNB.predict(X_test)\n",
    "\n",
    "methodName = \"NB\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_NB)  \n",
    "makeConfusionMatrixPic(6, methodName, dataSetName, classifierNB , X_test, y_test, predicted_NB)\n",
    "#makeSHAPreport(methodName, classifierNB)\n",
    "#makeSHAP_KERNELreport(methodName, classifierNB)\n",
    "#makeLIMEreport(methodName, classifierNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.     Neural Networks (e.g., Multilayer Perceptron)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000)  # Adjust the hidden_layer_sizes and max_iter as desired\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "methodName = \"Multilayer Perceptron\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_AdaBoost)  \n",
    "makeConfusionMatrixPic(7, methodName, dataSetName, mlp , X_test, y_test, y_pred)\n",
    "#makeSHAPreport(methodName, classifierAdaBoost)\n",
    "#makeSHAP_KERNELreport(methodName, classifierAdaBoost)\n",
    "#makeLIMEreport(methodName, classifierAdaBoost)\n",
    "\n",
    "accuracy = mlp.score(X_test_scaled, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.     AdaBoost \n",
    "classifierAdaBoost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=42) \n",
    "classifierAdaBoost.fit(X_train, y_train) \n",
    "predicted_AdaBoost = classifierAdaBoost.predict(X_test)\n",
    "\n",
    "\n",
    "methodName = \"AdaBoost\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_AdaBoost)  \n",
    "makeConfusionMatrixPic(8, methodName, dataSetName, classifierAdaBoost , X_test, y_test, predicted_AdaBoost)\n",
    "#makeSHAPreport(methodName, classifierAdaBoost)\n",
    "#makeSHAP_KERNELreport(methodName, classifierAdaBoost)\n",
    "#makeLIMEreport(methodName, classifierAdaBoost)\n",
    " \n",
    "accuracy = accuracy_score(y_test, predicted_AdaBoost)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.     Gaussian Processes\n",
    "# Create a Gaussian Process classifier with RBF kernel\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "#classifierGPC = GaussianProcessClassifier(kernel=kernel, random_state=42) \n",
    "classifierGPC = GaussianProcessClassifier( random_state=42) \n",
    "classifierGPC.fit(X_train, y_train) \n",
    "predicted_GPC = classifierGPC.predict(X_test)\n",
    "\n",
    "methodName = \"GaussianProcessClassifier\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_GPC)  \n",
    "makeConfusionMatrixPic(9, methodName, dataSetName, classifierGPC , X_test, y_test, predicted_GPC)\n",
    "#makeSHAPreport(methodName, classifierGPC)\n",
    "#makeSHAP_KERNELreport(methodName, classifierGPC)\n",
    "#makeLIMEreport(methodName, classifierGPC)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_GPC)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.     Extreme Gradient Boosting (XGBoost)\n",
    "classifierXGB = xgb.XGBClassifier() \n",
    "classifierXGB.fit(X_train, y_train) \n",
    "predicted_XGB = classifierXGB.predict(X_test)\n",
    "\n",
    "methodName = \"XGBClassifier\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_XGB)  \n",
    "makeConfusionMatrixPic(10, methodName, dataSetName, classifierXGB , X_test, y_test, predicted_XGB)\n",
    "#makeSHAPreport(methodName, classifierXGB)\n",
    "#makeSHAP_KERNELreport(methodName, classifierXGB)\n",
    "#makeLIMEreport(methodName, classifierXGB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_XGB)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.     CatBoost\n",
    "classifierCatBoost = CatBoostClassifier() \n",
    "classifierCatBoost.fit(X_train, y_train) \n",
    "predicted_CatBoost = classifierCatBoost.predict(X_test)\n",
    "\n",
    "methodName = \"CatBoostClassifier\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_CatBoost)  \n",
    "makeConfusionMatrixPic(11, methodName, dataSetName, classifierCatBoost , X_test, y_test, predicted_CatBoost)\n",
    "#makeSHAPreport(methodName, classifierCatBoost)\n",
    "#makeSHAP_KERNELreport(methodName, classifierCatBoost)\n",
    "#makeLIMEreport(methodName, classifierCatBoost)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_CatBoost)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.     LightGBM\n",
    "classifierLGB = lgb.LGBMClassifier() \n",
    "classifierLGB.fit(X_train, y_train) \n",
    "predicted_LGB = classifierLGB.predict(X_test)\n",
    " \n",
    "methodName = \"LightGBM\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_LGB)  \n",
    "makeConfusionMatrixPic(12, methodName, dataSetName, classifierLGB , X_test, y_test, predicted_LGB)\n",
    "#makeSHAPreport(methodName, classifierLGB)\n",
    "#makeSHAP_KERNELreport(methodName, classifierLGB)\n",
    "#makeLIMEreport(methodName, classifierLGB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_LGB)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.     Histogram-Based Gradient Boosting (HistGradientBoosting)\n",
    "classifierHGB = HistGradientBoostingClassifier() \n",
    "classifierHGB.fit(X_train, y_train) \n",
    "predicted_HGB = classifierHGB.predict(X_test)\n",
    "\n",
    "methodName = \"HistGradientBoostingClassifier\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_HGB)  \n",
    "makeConfusionMatrixPic(13, methodName, dataSetName, classifierHGB , X_test, y_test, predicted_HGB)\n",
    "#makeSHAPreport(methodName, classifierHGB)\n",
    "#makeSHAP_KERNELreport(methodName, classifierHGB)\n",
    "#makeLIMEreport(methodName, classifierHGB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_HGB)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14      Stacking: \n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = KNN()\n",
    "# Define the meta-classifier\n",
    "meta_clf = LogisticRegression()\n",
    "# Create the stacking classifier\n",
    "classifierStacking = StackingClassifier(estimators=[('rf', clf1), ('knn', clf2)], final_estimator=meta_clf)\n",
    "classifierStacking.fit(X_train, y_train) \n",
    "predicted_Stacking = classifierStacking.predict(X_test)\n",
    "\n",
    "methodName = \"Stacking(RF,KNN,LR)\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_Stacking)  \n",
    "makeConfusionMatrixPic(14, methodName, dataSetName, classifierStacking , X_test, y_test, predicted_Stacking)\n",
    "#makeSHAPreport(methodName, classifierStacking)\n",
    "#makeSHAP_KERNELreport(methodName, classifierStacking)\n",
    "#makeLIMEreport(methodName, classifierStacking)\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_Stacking)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15      Voting: \n",
    "# Define the individual classifiers\n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = KNN()\n",
    "clf3 = LogisticRegression()\n",
    "\n",
    "# Create the voting classifier\n",
    "classifierVoting = VotingClassifier(estimators=[('rf', clf1), ('knn', clf2), ('lr', clf3)], voting='hard')\n",
    "# Train the voting classifier\n",
    "classifierVoting.fit(X_train, y_train)\n",
    "# Make predictions on the test set\n",
    "predicted_Voting = classifierVoting.predict(X_test)\n",
    "\n",
    "methodName = \"Voting(RF,KNN,LR)\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_Voting)  \n",
    "makeConfusionMatrixPic(15, methodName, dataSetName, classifierVoting , X_test, y_test, predicted_Voting)\n",
    "#makeSHAPreport(methodName, classifierVoting)\n",
    "#makeSHAP_KERNELreport(methodName, classifierVoting)\n",
    "#makeLIMEreport(methodName, classifierVoting)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_Voting)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16      Bagging:\n",
    "# Create a base classifier\n",
    "base_clf = DecisionTreeClassifier() \n",
    "classifierBagging = BaggingClassifier(base_estimator=base_clf, n_estimators=10) \n",
    "classifierBagging.fit(X_train, y_train) \n",
    "predicted_Bagging = classifierBagging.predict(X_test)\n",
    "\n",
    "\n",
    "methodName = \"Bagging(DT)\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_Bagging)  \n",
    "makeConfusionMatrixPic(16, methodName, dataSetName, classifierBagging , X_test, y_test, predicted_Bagging)\n",
    "#makeSHAPreport(methodName, classifierBagging)\n",
    "#makeSHAP_KERNELreport(methodName, classifierBagging)\n",
    "#makeLIMEreport(methodName, classifierBagging)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_Bagging)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17      Extra Trees:  \n",
    "classifierExtraTree = ExtraTreesClassifier(n_estimators=100) \n",
    "classifierExtraTree.fit(X_train, y_train) \n",
    "predicted_ExtraTree = classifierExtraTree.predict(X_test)\n",
    "\n",
    "methodName = \"ExtraTree\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_ExtraTree)  \n",
    "makeConfusionMatrixPic(17, methodName, dataSetName, classifierExtraTree , X_test, y_test, predicted_ExtraTree)\n",
    "#makeSHAPreport(methodName, classifierExtraTree)\n",
    "#makeSHAP_KERNELreport(methodName, classifierExtraTree)\n",
    "#makeLIMEreport(methodName, classifierExtraTree)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_ExtraTree)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18      Ridge Classifier: \n",
    "classifierRidge = RidgeClassifier() \n",
    "classifierRidge.fit(X_train, y_train) \n",
    "predicted_Ridge = classifierRidge.predict(X_test)\n",
    " \n",
    "methodName = \"RidgeClassifier\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predicted_Ridge)  \n",
    "makeConfusionMatrixPic(18, methodName, dataSetName, classifierRidge , X_test, y_test, predicted_Ridge)\n",
    "#makeSHAPreport(methodName, classifierRidge)\n",
    "#makeSHAP_KERNELreport(methodName, classifierRidge)\n",
    "#makeLIMEreport(methodName, classifierRidge)\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predicted_Ridge)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine the number of rows in the table (excluding the header)\n",
    "num_rows = len(finalResultTable) - 1\n",
    "print(\"total Rows: {} -> Cols: {}\".format(len(finalResultTable), len(finalResultTable[0])))\n",
    "# Calculate the desired figure size based on the number of rows\n",
    "fig_width = 6  # Set the desired width of the figure\n",
    "fig_height = num_rows * 0.5  # Adjust the scaling factor to control the height\n",
    "\n",
    "fig, ax = mplot.subplots(figsize=(fig_width, fig_height)) \n",
    "table = mplot.table(cellText=finalResultTable, loc='center') \n",
    "\n",
    "table.auto_set_column_width(col=list(range(len(finalResultTable[0]))))\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12) \n",
    "table.scale(2.0, 2.0) \n",
    "\n",
    "dataSetString = \"Dataset:  {}, Total Records: {}, No. Features: {}\".format(dataSetName, totalRecords, fileData.__dataframe__().num_columns())\n",
    "target =\"Target Column Name: {} , No of Classes: {}\".format(columns[-1], len(fileData[columns[-1]].value_counts()))\n",
    "distributionOfTargetClassA = \"No of Training Records: {}  {:.2f} %\".format(len(X_train), ((len(X_train)/totalRecords) * 100.0))\n",
    " \n",
    "classBrecords = \"Healthy Persons in training data: {}       {:.1f} %\".format(negativeClass, ((positiveClass/len(y_train)) * 100.0))\n",
    "classArecords = \"Non-Healthy Persons in training data: {}       {:.1f} %\".format(positiveClass, ((positiveClass/len(y_train)) * 100.0))\n",
    "\n",
    "\n",
    "\n",
    "positiveClassTest =  0\n",
    "negativeClassTest = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClassTest += 1\n",
    "    if(i == 1):\n",
    "        positiveClassTest += 1\n",
    "classBrecordsTest = \"Healthy Persons in test data: {}       {:.1f} %\".format(negativeClassTest, ((negativeClassTest/len(y_test)) * 100.0))\n",
    "classArecordsTest = \"Non-Healthy Persons in test data: {}       {:.1f} %\".format(positiveClassTest, ((positiveClassTest/len(y_test)) * 100.0))\n",
    "\n",
    "\n",
    "\n",
    "distributionOfTargetClassB = \"No of Testing Records: {}  {:.2f} %\".format(len(X_test), ((len(X_test)/totalRecords) * 100.0))\n",
    "\n",
    "fig.text(-0.1, +0.10,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "fig.text(-0.1, +0.05,  target, horizontalalignment='left', wrap=False  , fontsize=12 )   \n",
    "fig.text(-0.1, 0.00,  distributionOfTargetClassA, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.03,  classBrecords, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.06,  classArecords, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "\n",
    "fig.text(-0.1, -0.12,  distributionOfTargetClassB, horizontalalignment='left', wrap=False  , fontsize=12 )  \n",
    "fig.text(-0.1, -0.15,  classBrecordsTest, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    "fig.text(-0.1, -0.17,  classArecordsTest, horizontalalignment='left', wrap=False , fontsize=12  )   \n",
    " \n",
    "mplot.axis('off')\n",
    "mplot.title(f'Final Result Table ({dataSetName})' , fontsize=16, fontweight='bold') \n",
    "picturePath = \"{}99.Final_Result_Table_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.savefig(picturePath,  dpi=300)\n",
    "mplot.show()\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#fpr, tpr, thresholds = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "mplot.figure()\n",
    "#mplot.plot(fpr, tpr, label=\"(area = {:.2f})\".format(logit_roc_auc))\n",
    "\n",
    "fprLR, tprLR, threshLR = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "#fprSVM, tprSVM, threshSVM = roc_curve(y_test, classifierSVM.predict_proba(X_test)[:,1])\n",
    "fprKNN, tprKNN, threshKNN = roc_curve(y_test, classifierKNN.predict_proba(X_test)[:,1])\n",
    "fprNB, tprNB, threshNB = roc_curve(y_test, classifierNB.predict_proba(X_test)[:,1])\n",
    " \n",
    "    \n",
    "# plotting    \n",
    "mplot.plot(fprLR, tprLR, linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "#mplot.plot(fprSVM, tprSVM, linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "mplot.plot(fprKNN, tprKNN, linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "mplot.plot(fprNB, tprNB, linestyle='--',color='red', label='Class 2 vs Rest')\n",
    "mplot.title('Multiclass ROC curve' ,fontsize=16, fontweight='bold')\n",
    "mplot.xlabel('False Positive Rate')\n",
    "mplot.ylabel('True Positive rate')\n",
    "mplot.legend(loc='best')\n",
    "mplot.savefig('Multiclass ROC',dpi=300);    \n",
    "mplot.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roundValue = 0.0\n",
    "for colName in fileData.__dataframe__().column_names():\n",
    "    fileColData = fileData[colName].astype(\"int\")\n",
    "    roundValue = round(fileData[colName].mean(), 2)\n",
    "\n",
    "    print(\"Column Name: {} -> MeanValue:{}\".format(colName, roundValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
