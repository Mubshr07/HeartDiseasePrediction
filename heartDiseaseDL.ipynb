{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import dalex as dx\n",
    "dx.__version__\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset, Preprocessing and making X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPredictionLessDimensions\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPredictionFewDimensions\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0)\n",
    "\n",
    "'''\n",
    "for columnName in fileData.__dataframe__().column_names():\n",
    "    #print(\"Column Name: {}, Len:{} \".format(columnName,len(fileData[columnName]))) \n",
    "    fileData[columnName] = np.floor(pd.to_numeric(fileData[columnName], errors='coerce')).astype('Int64') \n",
    "    meanValue = np.mean(fileData[columnName])\n",
    "    print(\"Column Name: {}, means:{} \".format(columnName, meanValue))\n",
    "    for i in range( len(fileData[columnName]) ):\n",
    "        print(\"----> Column Name: {}, Index:{} , Value:{} \".format(columnName, i, (fileData[columnName][i]) ))\n",
    "            #print(\"%%%%----> Column Name: {}, Index:{} , Value:{} \\n\\n\".format(columnName, i, str(fileData[columnName][i]) ))\n",
    "            fileData[columnName][i] = meanValue\n",
    "'''\n",
    "\n",
    "fileData = fileData.fillna(0)\n",
    "# 2. Handling Outliers:\n",
    "# 3. Feature Scaling:\n",
    "# 4. Encoding Categorical Variables:\n",
    "# 5. Feature Engineering:\n",
    "# 6. Handling Skewed Distributions:\n",
    "# 7. Dimensionality Reduction:\n",
    "# 8. Handling Imbalanced Classes:\n",
    "# 9. Normalization of Data:\n",
    "# 10. Data Splitting:\n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "dataSetResultDirectory += dataSetName\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n",
    "\n",
    "if(dataSetIndex == 0): \n",
    "    X = fileData[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']]  # Features\n",
    "    Y = fileData['target']  # Labels\n",
    "elif (dataSetIndex == 1): \n",
    "    #X = fileData[['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol', 'fasting blood sugar', 'resting ecg', 'max heart rate', 'exercise angina', 'oldpeak', 'ST slope']]  # Features\n",
    "    X = fileData[['fasting blood sugar', 'resting ecg', 'max heart rate', 'exercise angina', 'oldpeak']]  # Features LIME KNN\n",
    "    Y = fileData['target']  # Labels\n",
    "elif(dataSetIndex == 2): \n",
    "    X = fileData[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']]  # Features\n",
    "    Y = fileData['target']  # Labels\n",
    "elif(dataSetIndex == 3): \n",
    "    X = fileData.drop('TenYearCHD', axis=1)  # Features\n",
    "    Y = fileData['TenYearCHD']  # Labels\n",
    "elif(dataSetIndex == 4): \n",
    "    X = fileData.drop('CoronaryHeartDisease', axis=1)  # Features\n",
    "    Y = fileData['CoronaryHeartDisease']  # Labels\n",
    "else: \n",
    "    X = fileData.drop('CoronaryHeartDisease', axis=1)  # Features\n",
    "    Y = fileData['CoronaryHeartDisease']  # Labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Training 70% and Testing 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    " \n",
    "print(\"***************************************\") \n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "print(\"\\n X Train: Shape::\\n {}\".format(X_train.shape))\n",
    "print(\"\\n X Train: head::\\n {}\".format(X_train.columns))\n",
    "print(\"\\n X Test: head:: \\n{}\".format(X_test.columns))\n",
    "print(\"\\n Y Train: shape::\\n {}\".format(y_train.shape)) \n",
    "print(\"\\n Y Test: shape::\\n {}\".format(y_test.shape)) \n",
    " \n",
    "instance = np.array(X_test)  # Example: explaining the first instance in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape\n",
    "\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "normalizer  = tf.keras.layers.experimental.preprocessing.Normalization(input_shape=[p,])\n",
    "normalizer.adapt(X.to_numpy())\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.Input(shape=(p,)),\n",
    "    tf.keras.layers.Dense(p*2, activation='relu'),\n",
    "    tf.keras.layers.Dense(p*3, activation='relu'),\n",
    "    tf.keras.layers.Dense(p*2, activation='relu'),\n",
    "    tf.keras.layers.Dense(p, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.mae\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size=int(n/10), epochs=2000, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dx.Explainer(model, X, Y, label='CoronaryHeartDisease')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.model_performance()\n",
    "explainer.model_parts().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Shape of fileData: (37079, 51)\n",
    "Column Headings: Index(['SEQN', 'Gender', 'Age', 'Annual-Family-Income',\n",
    "       'Ratio-Family-Income-Poverty', 'X60-sec-pulse', 'Systolic', 'Diastolic',\n",
    "       'Weight', 'Height', 'Body-Mass-Index', 'White-Blood-Cells',\n",
    "       'Lymphocyte', 'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells',\n",
    "       'Hemoglobin', 'Mean-Cell-Vol', 'Mean-Cell-Hgb-Conc.',\n",
    "       'Mean-cell-Hemoglobin', 'Platelet-count', 'Mean-Platelet-Vol',\n",
    "       'Segmented-Neutrophils', 'Hematocrit', 'Red-Cell-Distribution-Width',\n",
    "       'Albumin', 'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose',\n",
    "       'GGT', 'Iron', 'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
    "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
    "       'Vigorous-work', 'Moderate-work', 'Health-Insurance', 'Diabetes',\n",
    "       'Blood-Rel-Diabetes', 'Blood-Rel-Stroke', 'CoronaryHeartDisease'],\n",
    "'''\n",
    "\n",
    "#explainer.predict_parts(X_train[50], type='shap').plot()\n",
    "lime_explanation = explainer.predict_surrogate(X_train.loc[1], mode='regression')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(title, dataSetName, y_tru, y_prd, figsize=(6, 6), ax=None):\n",
    "    #mplot.figure(figsize=figsize)\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = ['Healthy', 'Heart Disease']\n",
    "    rep = np.array( precision_recall_fscore_support(y_tru, y_prd) ).T\n",
    "    rep[0][0] *= 100.0\n",
    "    rep[0][1] *= 100.0\n",
    "    rep[0][2] *= 100.0\n",
    "    rep[1][0] *= 100.0\n",
    "    rep[1][1] *= 100.0\n",
    "    rep[1][2] *= 100.0\n",
    "    \n",
    "    ax = sns.heatmap(rep, annot=True, cmap='Blues', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
    "    ax.set_title(\"Classification Report {} Model\\n\\n\".format(title) ,fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('\\nDataset:{}'.format(dataSetName))\n",
    "    ax.xaxis.set_ticklabels(xticks)\n",
    "    ax.set_ylabel('Classes')\n",
    "    ax.yaxis.set_ticklabels(yticks)\n",
    "    \n",
    "    picturePath = \"{}ClassificationReport_{}_{}.png\".format(dataSetResultDirectory, title, dataSetName) \n",
    "    mplot.savefig(picturePath, dpi=300, bbox_inches='tight')\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "\n",
    "def makeConfusionMatrixPic(method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_estimator(classifierObj, X_test, y_test, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "    accuracyString =\"Accuracy {}: {}\".format(method, accuracy_score(y_test, predicted_Y)*100.0 ) \n",
    "    recallString =  'Recall {}: {}'.format(method, recall_score(y_test, predicted_Y) * 100.0)\n",
    "    #precisionString = 'Precision {}: {}'.format(method, precision_score(y_test, predicted_Y, average='weighted') * 100.0) \n",
    "    precisionString = 'Precision {}: {}'.format(method, precision_score(y_test, predicted_Y) * 100.0) \n",
    "    display.figure_.text(0.020, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "    display.figure_.text(0.020, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "    display.figure_.text(0.020, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "\n",
    "    display.figure_.text(0.020, -0.18,  dataSetString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "    picturePath = \"{}Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Model\n",
    "kerasSequentialModel = Sequential();\n",
    "\n",
    "kerasSequentialModel.add(Dense(13,  input_dim=13, activation='relu'))\n",
    "kerasSequentialModel.add(Dense(8, activation='relu'))\n",
    "kerasSequentialModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# loss='mean_sequared_error'\n",
    "#kerasSequentialModel.compile(optimizer=\"adam\", loss='binary_crossentropy')\n",
    "kerasSequentialModel.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kerasSequentialModel.fit(X_train, y_train, epochs=10, batch_size=10 )\n",
    "# Train model\n",
    "#kerasSequentialModel.fit(X_train, y_train, batch_size=30, epochs=10, verbose=False, validation_data=(X_test, y_test))\n",
    "kerasSequentialModel.fit(np.asarray(X_train).astype('float32') , np.asarray(y_train).astype('float32'), epochs=310, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray(X_train).astype('float32')\n",
    "#score = kerasSequentialModel.evaluate(np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32'), verbose=0)\n",
    "score = kerasSequentialModel.evaluate(np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32') )\n",
    "print('Test Score:', score)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Summary of neural network\n",
    "kerasSequentialModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictionsKerasSequential = kerasSequentialModel.predict(np.asarray(X_test).astype('float32'))\n",
    "\n",
    "print('Test Score:', predictionsKerasSequential)\n",
    "\n",
    "methodName = \"Keras Sequential Model\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predictionsKerasSequential)  \n",
    "makeConfusionMatrixPic(methodName, dataSetName, kerasSequentialModel , np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32'), predictionsKerasSequential) \n",
    "  \n",
    "confusionMatric = confusion_matrix(y_test, predictionsKerasSequential)\n",
    "print(\"{} ConfusionMatrix: {}\".format(methodName, confusionMatric))\n",
    "print(\"Accuracy {}:  {}\".format(methodName, accuracy_score(y_test, predictionsKerasSequential)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#fpr, tpr, thresholds = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "mplot.figure()\n",
    "#mplot.plot(fpr, tpr, label=\"(area = {:.2f})\".format(logit_roc_auc))\n",
    "\n",
    "fprLR, tprLR, threshLR = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "#fprSVM, tprSVM, threshSVM = roc_curve(y_test, classifierSVM.predict_proba(X_test)[:,1])\n",
    "fprKNN, tprKNN, threshKNN = roc_curve(y_test, classifierKNN.predict_proba(X_test)[:,1])\n",
    "fprNB, tprNB, threshNB = roc_curve(y_test, classifierNB.predict_proba(X_test)[:,1])\n",
    " \n",
    "    \n",
    "# plotting    \n",
    "mplot.plot(fprLR, tprLR, linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "#mplot.plot(fprSVM, tprSVM, linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "mplot.plot(fprKNN, tprKNN, linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "mplot.plot(fprNB, tprNB, linestyle='--',color='red', label='Class 2 vs Rest')\n",
    "mplot.title('Multiclass ROC curve')\n",
    "mplot.xlabel('False Positive Rate')\n",
    "mplot.ylabel('True Positive rate')\n",
    "mplot.legend(loc='best')\n",
    "mplot.savefig('Multiclass ROC',dpi=300);    \n",
    "mplot.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
