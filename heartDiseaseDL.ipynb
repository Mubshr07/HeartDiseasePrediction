{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "#import dalex as dx\n",
    "#print(dx.__version__) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Comsats\\python\\heartDiseasePrediction\\heartDiseaseDL.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseaseDL.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseaseDL.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Check if CUDA is available\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseaseDL.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available.')\n",
    "else:\n",
    "    print('GPU is not available.')\n",
    "\n",
    "# Check PyTorch version\n",
    "print('PyTorch version:', torch.__version__)\n",
    "\n",
    "# Check CUDA version\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "# Check cuDNN version\n",
    "if torch.cuda.is_available():\n",
    "    print('cuDNN version:', torch.backends.cudnn.version())\n",
    "\n",
    "# Verify GPU integration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # Use CUDA device\n",
    "else:\n",
    "    device = torch.device('cpu')   # Use CPU device\n",
    "\n",
    "# Perform a simple computation on the chosen device\n",
    "a = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "b = torch.tensor([4.0, 5.0, 6.0]).to(device)\n",
    "c = a + b\n",
    "\n",
    "print('Result:', c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset, Preprocessing and making X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 4\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score'], ] \n",
    "'''\n",
    "dataSetResultDirectory += (\"DatasetResults_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n",
    "'''\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Training 70% and Testing 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    " \n",
    "print(\"***************************************\") \n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "# Undersample the majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "'''\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Print the class distribution after undersampling\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))\n",
    "\n",
    "'''\n",
    "print(\"\\n X Train: Shape::\\n {}\".format(X_train.shape))\n",
    "print(\"\\n X Train: head::\\n {}\".format(X_train.columns))\n",
    "print(\"\\n X Test: head:: \\n{}\".format(X_test.columns))\n",
    "print(\"\\n Y Train: shape::\\n {}\".format(y_train.shape)) \n",
    "print(\"\\n Y Test: shape::\\n {}\".format(y_test.shape)) \n",
    "'''\n",
    "#print(\"\\n X Train: Info::\\n {}\".format(X_train.info())) \n",
    "\n",
    "\n",
    "instance = np.array(X_test)  # Example: explaining the first instance in the dataset\n",
    " \n",
    "dataSetResultDirectory += (\"DatasetResults_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(50,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test loss: {loss * 100}')\n",
    "print(f'Test accuracy: {accuracy * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities into class labels\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrixPic(fileID, method, dataSet, cm , X_test, y_test, predicted_Y):\n",
    "    \n",
    "    display = ConfusionMatrixDisplay.from_predictions(y_test,predicted_Y, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    \n",
    "         \n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "    accuracyValue = (accuracy_score(y_test, predicted_Y)*100.0) \n",
    "    recallValue = (recall_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "    precisionValue = (precision_score(y_test, predicted_Y) * 100.0) \n",
    "    f1Score = (f1_score(y_test, predicted_Y, average='weighted') * 100.0)\n",
    "\n",
    " \n",
    "    singleRowInTable = [] \n",
    "    singleRowInTable.append(fileID)\n",
    "    singleRowInTable.append(method)\n",
    "    singleRowInTable.append(\"{:.2f}\".format(accuracyValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(recallValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(precisionValue) )\n",
    "    singleRowInTable.append(\"{:.2f}\".format(f1Score) )\n",
    "\n",
    "    finalResultTable.append((singleRowInTable) )\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy : {:.2f} %\".format( accuracyValue) \n",
    "    recallString =  'Recall : {:.2f} %'.format(recallValue)\n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    precisionString = 'Precision : {:.2f} %'.format(precisionValue) \n",
    "    f1String = 'F1 Score : {:.2f} %'.format(f1Score) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "    \n",
    "    numberOfTrainingRecords = \"No of Training Records: {}  {:.2f} %\".format(len(X_train), ((len(X_train)/totalRecords) * 100.0))\n",
    "    numberOfTestingRecords = \"No of Testing Records: {}  {:.2f} %\".format(len(X_test), ((len(X_test)/totalRecords) * 100.0))\n",
    "\n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False , fontsize=12 )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False , fontsize=12 )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.21,  dataSetString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.25,  numberOfTrainingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    display.figure_.text(0.010, -0.29,  numberOfTestingRecords, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    \n",
    "    '''\n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False , fontsize=12 ) \n",
    "    '''\n",
    "    \n",
    "    picturePath = \"{}{}.Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, fileID, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeConfusionMatrixPic(52, \"DL TF, UnderSampling\", dataSetName, cm , X_test, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model summary as a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Read the model summary from the text file\n",
    "with open('model_summary.txt', 'r') as f:\n",
    "    summary_text = f.read()\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = mplot.subplots(figsize=(8, 6))\n",
    "\n",
    "# Set the text as the title of the plot\n",
    "ax.set_title('Model Summary')\n",
    "ax.text(0.5, 0.5, summary_text, va='center', ha='center', fontsize=10)\n",
    "\n",
    "# Remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "picturePath = \"{}{}.Model_Summary_{}_{}.png\".format(dataSetResultDirectory, 50, \"DL TF\", dataSetName)\n",
    "plot_model(model, to_file=picturePath, show_shapes=True, expand_nested=True, dpi=96, show_layer_names=True, show_dtype=False, rankdir='TB', show_layer_activations=True, show_trainable=True)\n",
    "picturePath = \"{}{}.Model_Summary_{}_{}.png\".format(dataSetResultDirectory, 51, \"DL TF\", dataSetName)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape\n",
    "\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "normalizer  = tf.keras.layers.experimental.preprocessing.Normalization(input_shape=[p,])\n",
    "normalizer.adapt(X.to_numpy())\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.Input(shape=(p,)),\n",
    "    tf.keras.layers.Dense(p*2, activation='relu'),\n",
    "    tf.keras.layers.Dense(p*3, activation='relu'),\n",
    "    tf.keras.layers.Dense(p*2, activation='relu'),\n",
    "    tf.keras.layers.Dense(p, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.mae\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size=int(n/10), epochs=2000, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dx.Explainer(model, X, Y, label='CoronaryHeartDisease')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.model_performance()\n",
    "explainer.model_parts().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Shape of fileData: (37079, 51)\n",
    "Column Headings: Index(['SEQN', 'Gender', 'Age', 'Annual-Family-Income',\n",
    "       'Ratio-Family-Income-Poverty', 'X60-sec-pulse', 'Systolic', 'Diastolic',\n",
    "       'Weight', 'Height', 'Body-Mass-Index', 'White-Blood-Cells',\n",
    "       'Lymphocyte', 'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells',\n",
    "       'Hemoglobin', 'Mean-Cell-Vol', 'Mean-Cell-Hgb-Conc.',\n",
    "       'Mean-cell-Hemoglobin', 'Platelet-count', 'Mean-Platelet-Vol',\n",
    "       'Segmented-Neutrophils', 'Hematocrit', 'Red-Cell-Distribution-Width',\n",
    "       'Albumin', 'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose',\n",
    "       'GGT', 'Iron', 'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
    "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
    "       'Vigorous-work', 'Moderate-work', 'Health-Insurance', 'Diabetes',\n",
    "       'Blood-Rel-Diabetes', 'Blood-Rel-Stroke', 'CoronaryHeartDisease'],\n",
    "'''\n",
    "\n",
    "#explainer.predict_parts(X_train[50], type='shap').plot()\n",
    "lime_explanation = explainer.predict_surrogate(X_train.loc[1], mode='regression')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCorrelationPic(correlationMatrix, numberOfTopFeatures, targetColumnName):     \n",
    "    correlation_values = correlationMatrix.abs()\n",
    "    sorted_correlation = correlation_values.unstack().sort_values(ascending=False)\n",
    "    sorted_correlation = sorted_correlation[sorted_correlation != 1.0]\n",
    "\n",
    "    num_features = numberOfTopFeatures  # Number of top features to display\n",
    "    top_features = sorted_correlation.head(num_features)\n",
    "    print(\"Top\", num_features, \"features based on correlation:\")\n",
    "    print(top_features)\n",
    " \n",
    "    top_features = correlationMatrix.abs().nlargest(numberOfTopFeatures, targetColumnName)[targetColumnName].index\n",
    "    top_correlation_matrix = correlationMatrix.loc[top_features, top_features]\n",
    "\n",
    "    mplot.figure(figsize=(10, 8))\n",
    "    sns.heatmap(top_correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    # Set the title of the plot\n",
    "    mplot.title('Correlation Heatmap ({})'.format(dataSetName)  ,fontsize=16, fontweight='bold')\n",
    "    \n",
    "    picturePath = \"{}0.1_Correlation_Matrix_DateSetName_{}.png\".format(dataSetResultDirectory, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "def plot_classification_report(title, dataSetName, y_tru, y_prd, figsize=(6, 6), ax=None):\n",
    "    #mplot.figure(figsize=figsize)\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = ['Healthy', 'Heart Disease']\n",
    "    rep = np.array( precision_recall_fscore_support(y_tru, y_prd) ).T\n",
    "    rep[0][0] *= 100.0\n",
    "    rep[0][1] *= 100.0\n",
    "    rep[0][2] *= 100.0\n",
    "    rep[1][0] *= 100.0\n",
    "    rep[1][1] *= 100.0\n",
    "    rep[1][2] *= 100.0\n",
    "    \n",
    "    ax = sns.heatmap(rep, annot=True, cmap='Blues', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
    "    ax.set_title(\"Classification Report {} Model\\n\\n\".format(title) ,fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('\\nDataset:{}'.format(dataSetName))\n",
    "    ax.xaxis.set_ticklabels(xticks)\n",
    "    ax.set_ylabel('Classes')\n",
    "    ax.yaxis.set_ticklabels(yticks)\n",
    "    \n",
    "    picturePath = \"{}ClassificationReport_{}_{}.png\".format(dataSetResultDirectory, title, dataSetName) \n",
    "    mplot.savefig(picturePath, dpi=300, bbox_inches='tight')\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "\n",
    "def makeLIMEreport(method, classifierObj):\n",
    "    class_names =  ['Healthy', 'Heart Disease']\n",
    "    listOfFeatures =  classifierObj.feature_names_in_\n",
    "    print(\"\\n\\n\\n Obj Features: {}, len: {} \\n\\n\".format(listOfFeatures, len(listOfFeatures)))\n",
    "    #explainer = lime_tabular.LimeTabularExplainer(np.array(X_train) , mode='regression', feature_names=listOfFeatures, verbose=True, class_names=class_names )\n",
    "    explainer = lime_tabular.LimeTabularExplainer(np.array(X_train) , mode='classification', feature_names=listOfFeatures, verbose=True, class_names=class_names )\n",
    "\n",
    "    num_instances = int(len(X_test)/4)\n",
    "    print(\"num_instances: {}\".format(num_instances))\n",
    "    # Generate explanations for each testing instance\n",
    "    explanations = []\n",
    "    for i in range(num_instances):\n",
    "        instan = instance[i]\n",
    "        explanation = explainer.explain_instance(instan, classifierObj.predict_proba, num_features=len(instan))\n",
    "        explanations.append(explanation)\n",
    "\n",
    "    #Extract the feature importance values from the explanations\n",
    "    feature_importances = np.mean([exp.as_map()[1] for exp in explanations], axis=0)\n",
    "    averageF = []\n",
    "    for singleFeature in feature_importances:\n",
    "        averageF.append(singleFeature[0])\n",
    "     \n",
    "    print(\"\\n\\nfeature_list:{}\".format(listOfFeatures))\n",
    "    print(\"averageF:{} \\n\\n\".format(averageF))\n",
    "\n",
    "\n",
    "    fig, ax = mplot.subplots()\n",
    "    ax.barh(listOfFeatures, averageF )\n",
    "    ax.set_ylabel('Feature List')\n",
    "    ax.set_xlabel('Average Importance')\n",
    "    ax.set_title(\"XAI LIME ({} Model)\".format(method) ,fontsize=16, fontweight='bold')\n",
    "     \n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(X_test))\n",
    "    explainerModelString = \"LIME Explainer Model: {}\".format(explainer.mode)\n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  explainerModelString, horizontalalignment='left', wrap=False )   \n",
    " \n",
    "\n",
    "    picturePath = \"{}XAI_LIME_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "def makeSHAPreport(method, model):\n",
    "    shap_explainer = shap.Explainer(model, X_train) \n",
    "    shap_values = shap_explainer.shap_values(X_test)  \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP Explainer ({} Model)\".format(method) ,fontsize=16, fontweight='bold')     \n",
    "\n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(X_test))\n",
    "    shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "    picturePath = \"{}XAI_SHAP_Explainer_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "def makeSHAP_KERNELreport(method, model):\n",
    "    shap_explainer = shap.KernelExplainer(model.predict_proba, X_train) \n",
    "    testingShape = X_test[0:10]\n",
    "    shap_values = shap_explainer.shap_values(testingShape)  \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP KernelExplainer ({} Model)\".format(method) ,fontsize=16, fontweight='bold')     \n",
    "\n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of Testing Set: {}\".format(len(testingShape))\n",
    "    shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "    picturePath = \"{}XAI_SHAP_KernelExplainer_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.savefig(picturePath,  dpi=300) \n",
    "    mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Model\n",
    "kerasSequentialModel = Sequential();\n",
    "\n",
    "kerasSequentialModel.add(Dense(13,  input_dim=13, activation='relu'))\n",
    "kerasSequentialModel.add(Dense(8, activation='relu'))\n",
    "kerasSequentialModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# loss='mean_sequared_error'\n",
    "#kerasSequentialModel.compile(optimizer=\"adam\", loss='binary_crossentropy')\n",
    "kerasSequentialModel.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kerasSequentialModel.fit(X_train, y_train, epochs=10, batch_size=10 )\n",
    "# Train model\n",
    "#kerasSequentialModel.fit(X_train, y_train, batch_size=30, epochs=10, verbose=False, validation_data=(X_test, y_test))\n",
    "kerasSequentialModel.fit(X_train, y_train, epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray(X_train).astype('float32')\n",
    "#score = kerasSequentialModel.evaluate(np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32'), verbose=0)\n",
    "score = kerasSequentialModel.evaluate(np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32') )\n",
    "print('Test Score:', score)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Summary of neural network\n",
    "kerasSequentialModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictionsKerasSequential = kerasSequentialModel.predict(np.asarray(X_test).astype('float32'))\n",
    "\n",
    "print('Test Score:', predictionsKerasSequential)\n",
    "\n",
    "methodName = \"Keras Sequential Model\" \n",
    "#plot_classification_report(methodName, dataSetName, y_test, predictionsKerasSequential)  \n",
    "makeConfusionMatrixPic(methodName, dataSetName, kerasSequentialModel , np.asarray(X_test).astype('float32'), np.asarray(y_test).astype('float32'), predictionsKerasSequential) \n",
    "  \n",
    "confusionMatric = confusion_matrix(y_test, predictionsKerasSequential)\n",
    "print(\"{} ConfusionMatrix: {}\".format(methodName, confusionMatric))\n",
    "print(\"Accuracy {}:  {}\".format(methodName, accuracy_score(y_test, predictionsKerasSequential)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#fpr, tpr, thresholds = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "mplot.figure()\n",
    "#mplot.plot(fpr, tpr, label=\"(area = {:.2f})\".format(logit_roc_auc))\n",
    "\n",
    "fprLR, tprLR, threshLR = roc_curve(y_test, classifierLR.predict_proba(X_test)[:,1])\n",
    "#fprSVM, tprSVM, threshSVM = roc_curve(y_test, classifierSVM.predict_proba(X_test)[:,1])\n",
    "fprKNN, tprKNN, threshKNN = roc_curve(y_test, classifierKNN.predict_proba(X_test)[:,1])\n",
    "fprNB, tprNB, threshNB = roc_curve(y_test, classifierNB.predict_proba(X_test)[:,1])\n",
    " \n",
    "    \n",
    "# plotting    \n",
    "mplot.plot(fprLR, tprLR, linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "#mplot.plot(fprSVM, tprSVM, linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "mplot.plot(fprKNN, tprKNN, linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "mplot.plot(fprNB, tprNB, linestyle='--',color='red', label='Class 2 vs Rest')\n",
    "mplot.title('Multiclass ROC curve')\n",
    "mplot.xlabel('False Positive Rate')\n",
    "mplot.ylabel('True Positive rate')\n",
    "mplot.legend(loc='best')\n",
    "mplot.savefig('Multiclass ROC',dpi=300);    \n",
    "mplot.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
