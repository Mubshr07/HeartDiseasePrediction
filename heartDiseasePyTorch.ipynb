{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "from torch.nn import functional as F\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    " \n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "from tabulate import tabulate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0] \n",
    "fileData.drop_duplicates(inplace=True) \n",
    "num_rows_after = fileData.shape[0] \n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))   \n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_PyTorch_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \" {}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)\n",
    " \n",
    "\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1]))\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "'''\n",
    "# Undersample the majority class\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "'''\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "#X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "X_test, y_test = smote.fit_resample(X_test, y_test) \n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#Y_train_scaled = scaler.transform(y_train)\n",
    "#Y_test_scaled = scaler.transform(y_test)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))  \n",
    " \n",
    " \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCorrelationPic(correlationMatrix, numberOfTopFeatures, targetColumnName):     \n",
    "    correlation_values = correlationMatrix.abs()\n",
    "    sorted_correlation = correlation_values.unstack().sort_values(ascending=False)\n",
    "    sorted_correlation = sorted_correlation[sorted_correlation != 1.0]\n",
    "\n",
    "    num_features = numberOfTopFeatures  # Number of top features to display\n",
    "    top_features = sorted_correlation.head(num_features)\n",
    "    print(\"Top\", num_features, \"features based on correlation:\")\n",
    "    print(top_features)\n",
    " \n",
    "    top_features = correlationMatrix.abs().nlargest(numberOfTopFeatures, targetColumnName)[targetColumnName].index\n",
    "    top_correlation_matrix = correlationMatrix.loc[top_features, top_features]\n",
    "\n",
    "    mplot.figure(figsize=(10, 8))\n",
    "    sns.heatmap(top_correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    # Set the title of the plot\n",
    "    mplot.title('Correlation Heatmap ({})'.format(dataSetName))\n",
    "    \n",
    "    picturePath = \"Correlation_Matrix_DateSetName_{}.png\".format(dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    mplot.show()\n",
    "    mplot.close()\n",
    "\n",
    "def makeConfusionMatrixPic(method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_estimator(classifierObj, X_test, y_test, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method))\n",
    "    display.ax_.set_xlabel('\\nPredicted Values')\n",
    "    display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy {}: {:.2f}\".format(method, accuracy_score(y_test, predicted_Y)*100.0 ) \n",
    "    recallString =  'Recall {}: {:.2f}'.format(method, recall_score(y_test, predicted_Y) * 100.0)\n",
    "    precisionString = 'Precision {}: {:.2f}'.format(method, precision_score(y_test, predicted_Y) * 100.0) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "\n",
    "    \n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    \n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.17,  dataSetString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "    picturePath = \"{}Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "#print(X_train_tensor)\n",
    "#print(y_train_tensor) \n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "print(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attention model\n",
    "class AttentionModel(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.W_query = nn.Linear(input_size, hidden_size)\n",
    "        self.W_key = nn.Linear(input_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        scores = self.V(torch.tanh(query + keys)).squeeze(2)\n",
    "        attention_weights = torch.softmax(scores, dim=2)\n",
    "        attended = torch.sum(x * attention_weights.unsqueeze(2), dim=2)\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(attended)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "model = AttentionModel(input_size, hidden_size)\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        #optimizer.zero_grad()\n",
    "        #print(inputs)\n",
    "        #print(labels)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to binary predictions\n",
    "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Attention mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        # Calculate attention weights\n",
    "        scores = self.attn(encoder_outputs)\n",
    "        scores = scores.view(-1, scores.size(1))\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        # Context vector\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
    "        return context_vector, attn_weights\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, output_dim, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, output_dim)\n",
    "    def forward(self, inputs, encoder_outputs):\n",
    "        # Embed input sequence\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, (hidden, _) = self.lstm(embedded)\n",
    "        context_vector, attn_weights = self.attention(encoder_outputs, hidden)\n",
    "        outputs = outputs + context_vector\n",
    "        combined_output = torch.cat((outputs, context_vector), dim=1)\n",
    "        output = self.decoder(combined_output)\n",
    "        return output, attn_weights\n",
    "\n",
    "# Custom Dataset Class\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_excel(data_path)\n",
    "        self.features = torch.tensor(data.drop('CoronaryHeartDisease', axis=1).values)\n",
    "        self.target = torch.tensor(data['CoronaryHeartDisease'].values).float()\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "# Train and evaluate\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(features)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss / len(train_loader)\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output, _\n",
    " \n",
    " \n",
    "# Load data\n",
    "\n",
    "#dataSetFilePath =  CoronaryHeartDisease\n",
    "#dataSetName = \"CardiacPrediction\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "dataset = HeartDataset(data_path) \n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print(train_dataset)\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(train_loader)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 38 # len(38)\n",
    "embedding_dim = 64\n",
    "hidden_size = 128\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Model and optimizer\n",
    "model = LSTMAttention(input_dim, embedding_dim, hidden_size, output_dim, num_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, attn_weights = model(features)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Step {i+1}/{len(train_loader)} | Loss: {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluate model\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset features length: 37079\n",
      "dataset Target length: 37079\n",
      "input Dim is: <bound method HeartDataset.__len__ of <__main__.HeartDataset object at 0x00000226AC5161A0>>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Comsats\\python\\heartDiseasePrediction\\heartDiseasePyTorch.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39m# Model and optimizer\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMAttention(input_dim, embedding_dim, hidden_size, output_dim, num_layers) \u001b[39m#.to(device)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n",
      "\u001b[1;32me:\\Comsats\\python\\heartDiseasePrediction\\heartDiseasePyTorch.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_dim, embedding_dim, hidden_size, output_dim, num_layers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39msuper\u001b[39m(LSTMAttention, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(input_dim, embedding_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLSTM(embedding_dim, hidden_size, num_layers, dropout\u001b[39m=\u001b[39mdropout)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Comsats/python/heartDiseasePrediction/heartDiseasePyTorch.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m Attention(hidden_size)\n",
      "File \u001b[1;32mc:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq \u001b[39m=\u001b[39m scale_grad_by_freq\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[0;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    " \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        scores = self.attn(encoder_outputs).view(-1, encoder_outputs.size(1))\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
    "        return context_vector, attn_weights\n",
    "\n",
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, output_dim, num_layers=1, dropout=0.5):\n",
    "        super(LSTMAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, output_dim)\n",
    "    '''\n",
    "    def forward(self, inputs, encoder_outputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, (hidden, _) = self.lstm(embedded)\n",
    "        context_vector, attn_weights = self.attention(encoder_outputs, hidden)\n",
    "        outputs = outputs + context_vector\n",
    "        combined_output = torch.cat((outputs, context_vector), dim=1)\n",
    "        output = self.decoder(combined_output)\n",
    "        return output, attn_weights\n",
    "    \n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs.long())\n",
    "        outputs, (hidden, _) = self.lstm(embedded)\n",
    "        context_vector, attn_weights = self.attention(outputs, hidden)\n",
    "        outputs = outputs + context_vector\n",
    "        combined_output = torch.cat((outputs, context_vector), dim=1)\n",
    "        output = self.decoder(combined_output)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_excel(data_path)\n",
    "        self.features = torch.tensor(data.drop('CoronaryHeartDisease', axis=1).values)\n",
    "        self.target = torch.tensor(data['CoronaryHeartDisease'].values).float()\n",
    "        print(\"dataset features length: {}\".format(len(self.features)))\n",
    "        print(\"dataset Target length: {}\".format(len(self.target)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        #features, target = features.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(features)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            #features, target = features.to(device), target.to(device)\n",
    "            output, _ = model(features)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)\n",
    "\n",
    "# Load data\n",
    "data_path = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "dataset = HeartDataset(data_path) \n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = dataset.__len__\n",
    "print(\"input Dim is: {}\".format(input_dim))\n",
    "embedding_dim = 64\n",
    "hidden_size = 128\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Model and optimizer\n",
    "model = LSTMAttention(input_dim, embedding_dim, hidden_size, output_dim, num_layers) #.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple neural network\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleModel()\n",
    "# Define a loss function (e.g., binary cross-entropy for binary classification)\n",
    "criterion = nn.BCELoss()\n",
    "# Define an optimizer (e.g., Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Example training loop\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(inputs)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Print or log the loss after each epoch\n",
    "    print(f'Epoch {epoch + 1}/{10}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define the Attention Mechanism Sequential Model\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=100, embedding_dim=64),\n",
    "    nn.LSTM(input_size=64, hidden_size=64, batch_first=True),\n",
    "    # Attention mechanism components\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Tanh(),\n",
    "    # Final layer\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ") \n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "'''\n",
    "model1 = nn.Sequential() \n",
    "model1.add(nn.Embedding(num_embeddings=100, embedding_dim=64))\n",
    "model1.add(nn.LSTM(input_size=64, hidden_size=64, batch_first=True))\n",
    "model1.add(nn.Linear(64, 64))\n",
    "model1.add(nn.Linear(64, 64))\n",
    "model1.add(nn.Linear(64, 1))\n",
    "model1.add(nn.Tanh())\n",
    "model1.add(nn.Linear(64, 32))\n",
    "model1.add(nn.Linear(32, 1))\n",
    "model1.add(nn.Sigmoid()) \n",
    "'''\n",
    "model1 = nn.Sequential( nn.Embedding(num_embeddings=100, embedding_dim=64) ,\n",
    "                        nn.LSTM(input_size=64, hidden_size=64, batch_first=True),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Linear(64, 1),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32, 1),\n",
    "                        nn.Sigmoid()\n",
    "                        ) \n",
    "\n",
    "\n",
    "\n",
    "model1.compile(optimizer= nn.optimizers.Adam(model1.parameters(), lr=0.001), loss=nn.BCELoss, metrics=['accuracy' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to binary predictions (assuming binary classification)\n",
    "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Draw confusion matrix\n",
    "cm = confusion_matrix(true_labels, binary_predictions)\n",
    "mplot.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "mplot.xlabel(\"Predicted\")\n",
    "mplot.ylabel(\"True\")\n",
    "mplot.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
