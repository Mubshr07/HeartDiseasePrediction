{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'memory_analyzer' from 'tensorflow.python.client' (c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_session\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m memory_analyzer\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profile\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Profiler\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'memory_analyzer' from 'tensorflow.python.client' (c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mplt \n",
    "import matplotlib.pyplot as mplot \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pickle \n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import random\n",
    "import dalex as dx \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, F1Score\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.backend import clear_session\n",
    "import sys\n",
    "from tensorflow.python.client import memory_analyzer\n",
    "from tensorflow.python.profiler import profile\n",
    "from tensorflow.python.profiler import Profiler\n",
    "\n",
    "\n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "f1_score = F1Score()\n",
    "auc = AUC()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fileData: (37079, 51)\n",
      "Column Headings: Index(['SEQN', 'Gender', 'Age', 'Annual-Family-Income',\n",
      "       'Ratio-Family-Income-Poverty', 'X60-sec-pulse', 'Systolic', 'Diastolic',\n",
      "       'Weight', 'Height', 'Body-Mass-Index', 'White-Blood-Cells',\n",
      "       'Lymphocyte', 'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells',\n",
      "       'Hemoglobin', 'Mean-Cell-Vol', 'Mean-Cell-Hgb-Conc.',\n",
      "       'Mean-cell-Hemoglobin', 'Platelet-count', 'Mean-Platelet-Vol',\n",
      "       'Segmented-Neutrophils', 'Hematocrit', 'Red-Cell-Distribution-Width',\n",
      "       'Albumin', 'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose',\n",
      "       'GGT', 'Iron', 'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Vigorous-work', 'Moderate-work', 'Health-Insurance', 'Diabetes',\n",
      "       'Blood-Rel-Diabetes', 'Blood-Rel-Stroke', 'CoronaryHeartDisease'],\n",
      "      dtype='object')\n",
      "Number of Records: 37079\n",
      "\n",
      "Number of Missing Values: 0\n",
      "Number of duplicate records removed: 0\n",
      "Shape of fileData: (37079, 51)\n",
      "Shape of fileData End: (37079, 51)\n",
      "\n",
      "\n",
      "columns of x:: 50 \n",
      "\n",
      " and features of X: Index(['SEQN', 'Gender', 'Age', 'Annual-Family-Income',\n",
      "       'Ratio-Family-Income-Poverty', 'X60-sec-pulse', 'Systolic', 'Diastolic',\n",
      "       'Weight', 'Height', 'Body-Mass-Index', 'White-Blood-Cells',\n",
      "       'Lymphocyte', 'Monocyte', 'Eosinophils', 'Basophils', 'Red-Blood-Cells',\n",
      "       'Hemoglobin', 'Mean-Cell-Vol', 'Mean-Cell-Hgb-Conc.',\n",
      "       'Mean-cell-Hemoglobin', 'Platelet-count', 'Mean-Platelet-Vol',\n",
      "       'Segmented-Neutrophils', 'Hematocrit', 'Red-Cell-Distribution-Width',\n",
      "       'Albumin', 'ALP', 'AST', 'ALT', 'Cholesterol', 'Creatinine', 'Glucose',\n",
      "       'GGT', 'Iron', 'LDH', 'Phosphorus', 'Bilirubin', 'Protein', 'Uric.Acid',\n",
      "       'Triglycerides', 'Total-Cholesterol', 'HDL', 'Glycohemoglobin',\n",
      "       'Vigorous-work', 'Moderate-work', 'Health-Insurance', 'Diabetes',\n",
      "       'Blood-Rel-Diabetes', 'Blood-Rel-Stroke'],\n",
      "      dtype='object')\n",
      "Shape of fileData: (37079, 51) , target Len:37079\n",
      "X: (37079, 50) , Y:(37079,)\n",
      "Target Column Name:: CoronaryHeartDisease \n",
      "\n",
      "\n",
      " X Train: Shape:: (29663, 50)\n",
      " X Test: Shape:: (7416, 50)\n",
      "Train DataSet Positive Class Records:: 1206\n",
      "Train DataSet Negative Class Records:: 28457\n",
      "Train DataSet Total Records:: 29663\n",
      "\n",
      "\n",
      "\n",
      "Test DataSet Positive Class Records:: 302\n",
      "Test DataSet Negative Class Records:: 7114\n",
      "Test DataSet Total Records:: 7416\n",
      "X_train shape: (29663, 50)   and dType: 50\n",
      "X_train_scaler shape: (29663, 50)   and dType: float64\n",
      "X_test_scaler shape: (7416, 50)   and dType: float64\n",
      "y_train shape: (29663,)   and dType: int64\n",
      "y_test_scaler shape: (7416, 1)   and dType: float32\n",
      "features shape: (29663, 50)   and dType: float64\n",
      "target shape: (29663, 1)   and dType: float64\n",
      "\n",
      "\n",
      "\n",
      " ------------------------------------------------------------\n",
      " Directory Path: ./DatasetResults_MLP_with_AttentionLayers_imBalanced_loaded_CardiacPrediction/ \n",
      " ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataSetIndex = 4\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_with_AttentionLayers_imBalanced_loaded_\" + dataSetName)\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n",
    "\n",
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "print(\"Target Column Name:: {} \\n\".format(fileData.__dataframe__().column_names()[-1])) \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))   \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0): negativeClass += 1\n",
    "    if(i == 1): positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "print(\"\\n\\n\") \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0): negativeClass += 1\n",
    "    if(i == 1): positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n",
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "print(\"X_train shape: {}   and dType: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"X_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n ------------------------------------------------------------\")\n",
    "print(\" Directory Path: {} \".format(dataSetResultDirectory))\n",
    "print(\" ------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes! The file path MLP Model exists.\n",
      "WARNING:tensorflow:From c:\\Users\\Mubashir Iqbal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "modelPath = Path(\"./DatasetResults_MLP_with_AttentionLayers_imBalancedCardiacPrediction/model02_trained_CardiacPrediction_(37079, 51)_20240109 1937_0.96percent.model\")\n",
    "shapExplainerPath = Path(\"./DatasetResults_MLP_with_AttentionLayers_imBalancedCardiacPrediction/ShapeExplainer_20240109_1946_95.98percent.pkl\")\n",
    "shapValuesPath = Path(\"./DatasetResults_MLP_with_AttentionLayers_imBalancedCardiacPrediction/ShapeExplainerValues_20240109_1946_95.98percent.pkl\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "if modelPath.exists(): print(f\"Yes! The file path MLP Model exists.\")\n",
    "else: print(f\"The file path MLP Model does not exist.\")\n",
    "\n",
    "\n",
    "loadedModel = 0\n",
    "loadedExplainer = 0\n",
    "loadedShapValues = 0\n",
    "\n",
    " \n",
    "# To load the saved explainer and SHAP values later:\n",
    "loaded_model = tf.keras.models.load_model(modelPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "Prediction Time for 1 (miliSec):  127.11548805236816\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Prediction Time for 500 (miliSec):  154.9229621887207\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batchSize  = 1\n",
    "single_instance = np.random.rand(batchSize, features.shape[1])\n",
    "single_instance = np.asarray(single_instance)\n",
    "start_time = time.time()\n",
    "predictions = loaded_model.predict(single_instance)  # Expand dimensions if needed\n",
    "end_time = time.time()\n",
    "prediction_time_single = end_time - start_time\n",
    "prediction_time_single_ms = prediction_time_single * 1000\n",
    "print(f\"Prediction Time for {single_instance.shape[0]} (miliSec): \", prediction_time_single_ms)\n",
    "\n",
    "\n",
    "batchSize  = 500\n",
    "single_instance = np.random.rand(batchSize, features.shape[1])\n",
    "single_instance = np.asarray(single_instance)\n",
    "start_time = time.time()\n",
    "predictions = loaded_model.predict(single_instance)  # Expand dimensions if needed\n",
    "end_time = time.time()\n",
    "prediction_time_single = end_time - start_time\n",
    "prediction_time_single_ms = prediction_time_single * 1000\n",
    "print(f\"Prediction Time for {single_instance.shape[0]} (miliSec): \", prediction_time_single_ms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1213sdfasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shapExplainerPath.exists(): print(f\"Yes! The file path SHAP Explainer exists.\")\n",
    "else: print(f\"The file path SHAP Explainer does not exist.\")\n",
    "if shapValuesPath.exists(): print(f\"Yes! The file path SHAP Values exists.\")\n",
    "else: print(f\"The file path SHAP Values does not exist.\")\n",
    "\n",
    "\n",
    "\n",
    "# Register the AttentionLayer class before loading the model\n",
    "@tf.keras.utils.register_keras_serializable(package=\"AttentionLayer\")\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q, v = inputs\n",
    "        attention = tf.keras.layers.Attention()([q, v])\n",
    "        return attention\n",
    "     \n",
    "\n",
    "# To load the saved explainer and SHAP values later:\n",
    "#with open((\"./DatasetResults_MLP_with_AttentionLayers_imBalancedCardiacPrediction/ShapeExplainer_20240109_1946_95.98percent.pkl\"), 'rb') as explainer_file:\n",
    "#     loadedExplainer = pickle.load(explainer_file)\n",
    "\n",
    "with open(shapValuesPath, 'rb') as shap_values_file:\n",
    "     loadedShapValues = pickle.load(shap_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = loadedShapValues\n",
    "feature_names = X.columns.tolist()\n",
    "percentage = 0.2\n",
    "numberOfFeatures = int(len(features) *percentage)\n",
    "print(\"Total Features: {} -> Selected for SHAP:: {}\".format(len(features), numberOfFeatures))\n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    "\n",
    "numberOftest = int(len(X_test_scaler) * percentage)\n",
    "print(\"Total Test: {} -> Selected for SHAP:: {}\".format(len(X_test_scaler), numberOftest))\n",
    "testForShap = X_test_scaler[0:len(featuresForShap)]\n",
    "testForShap = X_test_scaler[0:numberOftest]\n",
    "#print(\" testForShap Name: {}\".format(  testForShap))\n",
    "\n",
    "\n",
    "avg_shap_values = np.mean(shap_values, axis=0)\n",
    "# Get indices that would sort the average SHAP values\n",
    "sorted_indices = np.argsort(avg_shap_values)[::-1]\n",
    "# Arrange feature names and average SHAP values in ascending order\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "sorted_avg_shap_values = avg_shap_values[sorted_indices]\n",
    "# Print sorted feature names and average SHAP values\n",
    "print(\"\\n\\nSorted Average SHAP values:\")\n",
    "for i in range(len(sorted_feature_names)):\n",
    "    print(f\"{sorted_feature_names[i]}, {sorted_avg_shap_values[i]}\")\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------\n",
    "    \n",
    "howManyFeatures = 20\n",
    "\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = sorted_indices[:howManyFeatures]\n",
    "# Select only the top 15 features and corresponding SHAP values\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary Bar\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False) \n",
    " \n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_Sorted_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"violin\", show=False, plot_size=(12, 6))\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_Violin_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"layered_violin\", show=False )\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_layered_violin_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select the top 10 features\n",
    "top_10_feature_indices = top_features_indices #sorted_indices[:15]\n",
    "top_10_feature_names = [feature_names[i] for i in top_10_feature_indices]\n",
    "top_10_shap_values = shap_values[:, top_10_feature_indices]\n",
    "# Create a DataFrame for visualization\n",
    "df_top_10 = pd.DataFrame(data=top_10_shap_values, columns=top_10_feature_names)\n",
    "# Plotting with Seaborn's violinplot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.violinplot(data=df_top_10, inner=\"quartile\", palette=\"muted\") \n",
    "plt.title('MLP Model with SHAP (XAI) Violin Plot')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Violinplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x-axis range\n",
    "x_axis_range = (-0.025, 0.025)  # Adjust the range as needed\n",
    "# Plotting with Seaborn's kdeplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.kdeplot(data=df_top_10[feature], label=feature, common_norm=False, common_grid=True, fill=True, clip=x_axis_range)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) KDE Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_KDEplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a Bubble Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, feature in enumerate(top_10_feature_names):\n",
    "    size = np.abs(df_top_10[feature]) * 100  # Adjust the scale as needed\n",
    "    plt.scatter(x=[i] * len(df_top_10), y=df_top_10[feature], s=size, label=feature, alpha=0.6)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Bubble Chart')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubbleChart_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the top 10 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_top_10, orient='v', palette='Set2')\n",
    "\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Box Plot')\n",
    "plt.xlabel('SHAP Values')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(range(len(top_10_feature_names)), top_10_feature_names, rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BoxPlot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an area chart for all SHAP values of the top 10 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "for feature in top_10_feature_names:\n",
    "    sns.lineplot(x=range(df_top_10.shape[0]), y=df_top_10[feature], label=feature)\n",
    " \n",
    "plt.title('MLP Model with SHAP (XAI) Area Chart')\n",
    "\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_AreaChart_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming your features are in a pandas DataFrame, you can access feature names\n",
    "feature_names = X.columns.tolist()\n",
    "# Get the top 3 features based on their absolute average SHAP values\n",
    "top_3_feature_indices = np.argsort(np.abs(shap_values.mean(0)))[-3:]\n",
    "top_3_feature_names = [feature_names[i] for i in top_3_feature_indices]\n",
    "# Extract SHAP values for the top 3 features\n",
    "shap_values_top_3 = shap_values[:, top_3_feature_indices]\n",
    "# Create a DataFrame for visualization\n",
    "df_top_3 = pd.DataFrame(data=shap_values_top_3, columns=top_3_feature_names)\n",
    "\n",
    "# Create a 3D bubble plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "\n",
    "# Scatter plot with adjusted size and color\n",
    "scatter = ax.scatter(df_top_3[top_3_feature_names[0]], df_top_3[top_3_feature_names[1]], df_top_3[top_3_feature_names[2]],\n",
    "                     s=1500 * np.abs(df_top_3.mean(axis=1)),  # Adjust the size\n",
    "                     c=df_top_3.mean(axis=1), cmap='viridis', alpha=0.9, edgecolors='w', linewidth=0.8)  # Adjust the color\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(top_3_feature_names[0])\n",
    "ax.set_ylabel(top_3_feature_names[1])\n",
    "ax.set_zlabel(top_3_feature_names[2])\n",
    "ax.set_title('3D Bubble Plot of Top 3 Features')\n",
    "plt.title('MLP Model with SHAP (XAI) 3D Bubble Plot')\n",
    "\n",
    "\n",
    "# Add colorbar\n",
    "colorbar = plt.colorbar(scatter, ax=ax, label='Average SHAP Value')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_BubblePlot_3D_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory, dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Seaborn's swarmplot (Beeswarm plot) \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.stripplot(data=df_top_10, palette=\"muted\", size=3, jitter=True)  # Use jitter=True for strip plot\n",
    "\n",
    "plt.title('MLP Model with SHAP (XAI) Strip Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Stripplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Seaborn's swarmplot (Beeswarm plot) \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.swarmplot(data=df_top_10, palette=\"muted\", size=3) \n",
    "plt.title('MLP Model with SHAP (XAI) Beeswarm Plot')\n",
    "plt.ylabel('SHAP Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "picturePath = \"{}XAI_SHAP_Explainer_SNS_Beeswarmplot_{}_numberOfSamples_{}.png\".format(dataSetResultDirectory,  dataSetName, len(testForShap))\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
